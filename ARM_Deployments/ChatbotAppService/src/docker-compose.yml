version: '3.8'

services:
  # Main Node.js chatbot application
  chatbot-app:
    build: .
    ports:
      - "3000:3000"
    environment:
      - NODE_ENV=development
      - PORT=3000
      - LLM_ENDPOINT=http://ollama-sidecar:11434
      - LLM_MODEL=tinyllama:latest
      - LLM_TIMEOUT=30000
      - MAX_CONVERSATION_LENGTH=20
    depends_on:
      ollama-sidecar:
        condition: service_healthy
    restart: unless-stopped
    volumes:
      - .:/app
      - /app/node_modules
    networks:
      - chatbot-network
    healthcheck:
      test: ["CMD", "node", "-e", "require('http').get('http://localhost:3000/health', (res) => { process.exit(res.statusCode === 200 ? 0 : 1) })"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 40s

  # Ollama LLM sidecar service
  ollama-sidecar:
    image: ollama/ollama:latest
    ports:
      - "11434:11434"
    environment:
      - OLLAMA_ORIGINS=*
      - OLLAMA_HOST=0.0.0.0:11434
    volumes:
      - ollama-data:/root/.ollama
    restart: unless-stopped
    networks:
      - chatbot-network
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:11434/api/tags"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 60s
    # Pull the model on startup
    command: >
      sh -c "
        ollama serve &
        echo 'Waiting for Ollama to start...' &&
        sleep 30 &&
        echo 'Pulling TinyLlama model...' &&
        ollama pull tinyllama:latest &&
        echo 'Model ready!' &&
        wait
      "

  # Optional: Redis for production conversation storage
  # redis:
  #   image: redis:7-alpine
  #   ports:
  #     - "6379:6379"
  #   volumes:
  #     - redis-data:/data
  #   restart: unless-stopped
  #   networks:
  #     - chatbot-network
  #   command: redis-server --appendonly yes

volumes:
  ollama-data:
    driver: local
  # redis-data:
  #   driver: local

networks:
  chatbot-network:
    driver: bridge